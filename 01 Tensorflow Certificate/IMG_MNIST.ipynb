{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IMG_MNIST.ipynb","provenance":[],"collapsed_sections":["Zs0h0otXXL4N","0tsjR8K05345","xzdAxI3O5vvx","KjyXnDT3TWd9","81e29BP6n4v1"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/btcnhung1299/tf-practice/blob/master/TF_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"mmtiaOnLdKyG"},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TQOPceUgdRXK"},"source":["# Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"H5ctqa_2dSgX"},"source":["## Load dataset"]},{"cell_type":"markdown","metadata":{"id":"uGsD6fVSfFaG"},"source":["We can apply the same flow for:\n","\n","*   MNIST\n","*   FashionMNIST\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zs0h0otXXL4N"},"source":["### Tensorflow dataset"]},{"cell_type":"code","metadata":{"id":"jFGb0Za0dTik","colab":{"base_uri":"https://localhost:8080/","height":586},"outputId":"eb0a5b59-f592-4ed0-8de8-0458b5957f18"},"source":["(ds_train, ds_test), ds_info = tfds.load(\"fashion_mnist\", split=[\"train\", \"test\"],\n","                                         as_supervised=True, shuffle_files=True,\n","                                         with_info=True)\n","ds_info"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tfds.core.DatasetInfo(\n","    name='fashion_mnist',\n","    version=3.0.0,\n","    description='Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.',\n","    homepage='https://github.com/zalandoresearch/fashion-mnist',\n","    features=FeaturesDict({\n","        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n","        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n","    }),\n","    total_num_examples=70000,\n","    splits={\n","        'test': 10000,\n","        'train': 60000,\n","    },\n","    supervised_keys=('image', 'label'),\n","    citation=\"\"\"@article{DBLP:journals/corr/abs-1708-07747,\n","      author    = {Han Xiao and\n","                   Kashif Rasul and\n","                   Roland Vollgraf},\n","      title     = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n","                   Algorithms},\n","      journal   = {CoRR},\n","      volume    = {abs/1708.07747},\n","      year      = {2017},\n","      url       = {http://arxiv.org/abs/1708.07747},\n","      archivePrefix = {arXiv},\n","      eprint    = {1708.07747},\n","      timestamp = {Mon, 13 Aug 2018 16:47:27 +0200},\n","      biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07747},\n","      bibsource = {dblp computer science bibliography, https://dblp.org}\n","    }\"\"\",\n","    redistribution_info=,\n",")"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"0tsjR8K05345"},"source":["### Keras dataset"]},{"cell_type":"code","metadata":{"id":"eavt3mhJT5rH"},"source":["(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","train_images = train_images.reshape(*train_images.shape, 1)\n","test_images = test_images.reshape(*test_images.shape, 1)\n","\n","ds_train = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n","ds_test = tf.data.Dataset.from_tensor_slices((test_images, test_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzdAxI3O5vvx"},"source":["## Batchify dataset"]},{"cell_type":"code","metadata":{"id":"4qkKPm4lYv2E"},"source":["input_shape = next(iter(ds_train))[0].shape\n","num_classes = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUYNMSLv5zN9","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"a3e789ac-6807-44e6-c1cb-60f86df81355"},"source":["BATCH_SIZE = 128\n","ds_train = ds_train.batch(BATCH_SIZE)\n","ds_test = ds_test.batch(BATCH_SIZE)\n","ds_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<DatasetV1Adapter shapes: ((None, 28, 28, 1), (None,)), types: (tf.uint8, tf.int64)>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"KjyXnDT3TWd9"},"source":["# Model Architecture"]},{"cell_type":"code","metadata":{"id":"yUh9odc3d20j"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n","from tensorflow.keras.layers.experimental.preprocessing import Rescaling"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Af5sZbUHk4Lu","colab":{"base_uri":"https://localhost:8080/","height":583},"outputId":"b6c50da4-94ee-42b1-cd72-8109fbed0be9"},"source":["model = Sequential()\n","\n","model.add(Rescaling(scale=1./255, input_shape=input_shape))\n","\n","model.add(Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=input_shape))\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.1))\n","\n","model.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n","model.add(MaxPool2D(pool_size=(2, 2)))\n","model.add(Dropout(0.2))\n","\n","model.add(Flatten())\n","model.add(Dense(128, activation=\"relu\"))\n","model.add(Dense(num_classes, activation=\"softmax\"))\n","\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","rescaling_5 (Rescaling)      (None, 28, 28, 1)         0         \n","_________________________________________________________________\n","conv2d_15 (Conv2D)           (None, 28, 28, 16)        160       \n","_________________________________________________________________\n","max_pooling2d_15 (MaxPooling (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","dropout_15 (Dropout)         (None, 14, 14, 16)        0         \n","_________________________________________________________________\n","conv2d_16 (Conv2D)           (None, 14, 14, 32)        4640      \n","_________________________________________________________________\n","max_pooling2d_16 (MaxPooling (None, 7, 7, 32)          0         \n","_________________________________________________________________\n","dropout_16 (Dropout)         (None, 7, 7, 32)          0         \n","_________________________________________________________________\n","conv2d_17 (Conv2D)           (None, 7, 7, 128)         36992     \n","_________________________________________________________________\n","max_pooling2d_17 (MaxPooling (None, 3, 3, 128)         0         \n","_________________________________________________________________\n","dropout_17 (Dropout)         (None, 3, 3, 128)         0         \n","_________________________________________________________________\n","flatten_5 (Flatten)          (None, 1152)              0         \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 128)               147584    \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 10)                1290      \n","=================================================================\n","Total params: 190,666\n","Trainable params: 190,666\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"81e29BP6n4v1"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"QgoXPIacm-94","colab":{"base_uri":"https://localhost:8080/","height":700},"outputId":"5ccb5d7c-02e2-4cd5-98ca-328f38ded8bc"},"source":["model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n","model.fit(ds_train, epochs=20, validation_data=ds_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","469/469 [==============================] - 15s 33ms/step - loss: 0.6343 - acc: 0.7639 - val_loss: 0.4253 - val_acc: 0.8439\n","Epoch 2/20\n","469/469 [==============================] - 16s 33ms/step - loss: 0.3842 - acc: 0.8593 - val_loss: 0.3350 - val_acc: 0.8765\n","Epoch 3/20\n","469/469 [==============================] - 16s 34ms/step - loss: 0.3294 - acc: 0.8800 - val_loss: 0.3060 - val_acc: 0.8903\n","Epoch 4/20\n","469/469 [==============================] - 16s 34ms/step - loss: 0.3000 - acc: 0.8880 - val_loss: 0.2867 - val_acc: 0.8961\n","Epoch 5/20\n","469/469 [==============================] - 16s 33ms/step - loss: 0.2782 - acc: 0.8969 - val_loss: 0.2753 - val_acc: 0.8998\n","Epoch 6/20\n","469/469 [==============================] - 15s 33ms/step - loss: 0.2629 - acc: 0.9015 - val_loss: 0.2750 - val_acc: 0.9021\n","Epoch 7/20\n","469/469 [==============================] - 15s 32ms/step - loss: 0.2512 - acc: 0.9066 - val_loss: 0.2630 - val_acc: 0.9059\n","Epoch 8/20\n","469/469 [==============================] - 16s 33ms/step - loss: 0.2402 - acc: 0.9114 - val_loss: 0.2414 - val_acc: 0.9126\n","Epoch 9/20\n","469/469 [==============================] - 15s 33ms/step - loss: 0.2324 - acc: 0.9133 - val_loss: 0.2484 - val_acc: 0.9094\n","Epoch 10/20\n","469/469 [==============================] - 15s 33ms/step - loss: 0.2237 - acc: 0.9160 - val_loss: 0.2472 - val_acc: 0.9106\n","Epoch 11/20\n","469/469 [==============================] - 16s 33ms/step - loss: 0.2166 - acc: 0.9187 - val_loss: 0.2326 - val_acc: 0.9170\n","Epoch 12/20\n","469/469 [==============================] - 15s 33ms/step - loss: 0.2112 - acc: 0.9211 - val_loss: 0.2317 - val_acc: 0.9188\n","Epoch 13/20\n","469/469 [==============================] - 15s 33ms/step - loss: 0.2060 - acc: 0.9235 - val_loss: 0.2287 - val_acc: 0.9187\n","Epoch 14/20\n","469/469 [==============================] - 15s 33ms/step - loss: 0.2019 - acc: 0.9247 - val_loss: 0.2319 - val_acc: 0.9181\n","Epoch 15/20\n","469/469 [==============================] - 15s 32ms/step - loss: 0.2015 - acc: 0.9247 - val_loss: 0.2304 - val_acc: 0.9196\n","Epoch 16/20\n","469/469 [==============================] - 15s 33ms/step - loss: 0.1962 - acc: 0.9264 - val_loss: 0.2258 - val_acc: 0.9234\n","Epoch 17/20\n","469/469 [==============================] - 15s 33ms/step - loss: 0.1928 - acc: 0.9279 - val_loss: 0.2253 - val_acc: 0.9214\n","Epoch 18/20\n","469/469 [==============================] - 16s 33ms/step - loss: 0.1904 - acc: 0.9285 - val_loss: 0.2277 - val_acc: 0.9199\n","Epoch 19/20\n","469/469 [==============================] - 16s 33ms/step - loss: 0.1867 - acc: 0.9301 - val_loss: 0.2274 - val_acc: 0.9205\n","Epoch 20/20\n","469/469 [==============================] - 16s 33ms/step - loss: 0.1836 - acc: 0.9302 - val_loss: 0.2141 - val_acc: 0.9247\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f02c95a1390>"]},"metadata":{"tags":[]},"execution_count":29}]}]}